{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d3c541",
   "metadata": {},
   "source": [
    "# WEBSCRAPPING ASSIGNMENT2 USING SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c230aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now import all the required libraberies\n",
    "import selenium                    #libraberies that is used to work with selenium\n",
    "import pandas as pd                #to create dataframe\n",
    "from selenium import webdriver     #importing webdriver module from selenium to open up automated chrome window\n",
    "import warnings                    #to ignore any sort of warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time                        #use to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7364ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Webbrowser\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826f2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8270d37",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6289b",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af84b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input\") # job search bar\n",
    "search_field_designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690abbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field_location=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da50036",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "location_list=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets extract all the tags having the job-titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\") #locating web element of title\n",
    "title_tags[0:4] #using the range to print only top 4 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8bb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "\n",
    "#so we will run a loop to iterate over the tags extracted above and extract the text inside them.\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")#locating web element of company name\n",
    "companies_tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc72653",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=[]\n",
    "for i in companies_tags:               #ieterating over web element of company name\n",
    "    company_name=i.text               #extracting text from each web element \n",
    "#    print(company_name)\n",
    "    company_names.append(company_name) #appending each extected text into empty list\n",
    "company_names[0:4]                     #printing top 4 results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ff0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets extract all the tags having the experiene required data\n",
    "experiance_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "experiance_tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiance_yrs=[]\n",
    "for i in experiance_tags:               #ieterating over web element of company name\n",
    "    experiance=i.text               #extracting text from each web element \n",
    "#    print(company_name)\n",
    "    experiance_yrs.append(experiance) #appending each extected text into empty list\n",
    "experiance_yrs[0:4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "location_tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a14ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=[]\n",
    "for i in location_tags:               #ieterating over web element of company name\n",
    "    location=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    locations.append(location) #appending each extected text into empty list\n",
    "locations[0:4]\n",
    "print(locations)\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_titles),len(company_names),len(experiance_yrs),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['titles']=job_titles\n",
    "jobs['company']=company_names\n",
    "jobs['experience_required']=experiance_yrs\n",
    "jobs['location']=locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to close automated chrome window\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719396d",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68561289",
   "metadata": {},
   "source": [
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dceda6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input\") # job search bar\n",
    "search_field_designation.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab86730",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field_location=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "location_list=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1818b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets extract all the tags having the job-titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\") #locating web element of title\n",
    "title_tags[0:4] #using the range to print only top 4 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea77a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "\n",
    "#so we will run a loop to iterate over the tags extracted above and extract the text inside them.\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")#locating web element of company name\n",
    "companies_tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=[]\n",
    "for i in companies_tags:               #ieterating over web element of company name\n",
    "    company_name=i.text               #extracting text from each web element \n",
    "#    print(company_name)\n",
    "    company_names.append(company_name) #appending each extected text into empty list\n",
    "company_names[0:4]                     #printing top 4 results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca33f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets extract all the tags having the experiene required data\n",
    "experiance_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "experiance_tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b509842",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiance_yrs=[]\n",
    "for i in experiance_tags:               #ieterating over web element of company name\n",
    "    experiance=i.text               #extracting text from each web element \n",
    "#    print(company_name)\n",
    "    experiance_yrs.append(experiance) #appending each extected text into empty list\n",
    "experiance_yrs[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d59744",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "location_tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305aca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=[]\n",
    "for i in location_tags:               #ieterating over web element of company name\n",
    "    location=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    locations.append(location) #appending each extected text into empty list\n",
    "locations[0:4]\n",
    "print(locations)\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_titles),len(company_names),len(experiance_yrs),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed02625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['titles']=job_titles\n",
    "jobs['company']=company_names\n",
    "jobs['experience_required']=experiance_yrs\n",
    "jobs['location']=locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1741ee",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fad282",
   "metadata": {},
   "source": [
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c5414",
   "metadata": {},
   "source": [
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04693c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input\") # job search bar\n",
    "search_field_designation.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a20fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath(\"//div[@class='qsbSubmit']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=[]\n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "location_tags[0:4]\n",
    "\n",
    "for i in location_tags:               #ieterating over web element of company name\n",
    "    location=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    locations.append(location) #appending each extected text into empty list\n",
    "locations[0:4]\n",
    "print(locations)\n",
    "len(locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4bb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi salary']\")\n",
    "salary_tags[0:4]\n",
    "\n",
    "salarys=[]\n",
    "for i in salary_tags:               #ieterating over web element of company name\n",
    "    salary=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    salarys.append(salary) #appending each extected text into empty list\n",
    "salarys[0:4]\n",
    "print(salarys)\n",
    "len(salarys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb034143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['locations']=locations\n",
    "jobs['salarys']=salarys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a90db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f59551",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee97006",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "    \n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "        \n",
    "2. Enter “sunglasses” in the search field where “search for products, brands andmore” is written and click the search icon\n",
    "\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data as usual.\n",
    "ASSIGNMENT 2\n",
    "\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom ofthe page , then click on it.\n",
    "\n",
    "5. Now scrape data from this page as usual\n",
    "\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cac94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for Sunglasses search bar\n",
    "search_field_Sunglasses=driver.find_element_by_class_name(\"_3704LK\") # job search bar\n",
    "search_field_Sunglasses.send_keys(\"Sunglasses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9559ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "ProductDescriptions=[]\n",
    "Prices=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ef174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "print(Brands)\n",
    "len(Brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProductDescription_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "ProductDescription_tags[0:4]\n",
    "\n",
    "for i in ProductDescription_tags:               #ieterating over web element of ProductDescription\n",
    "    ProductDescription=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    ProductDescriptions.append(ProductDescription) #appending each extected text into empty list\n",
    "ProductDescriptions[0:4]\n",
    "print(ProductDescriptions)\n",
    "len(ProductDescriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede65237",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "print(Prices)\n",
    "len(Prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "print(Brands)\n",
    "len(Brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProductDescription_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "ProductDescription_tags[0:4]\n",
    "\n",
    "for i in ProductDescription_tags:               #ieterating over web element of ProductDescription\n",
    "    ProductDescription=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    ProductDescriptions.append(ProductDescription) #appending each extected text into empty list\n",
    "ProductDescriptions[0:4]\n",
    "print(ProductDescriptions)\n",
    "\n",
    "len(ProductDescriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "print(Prices)\n",
    "len(Prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed326483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c04c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "print(Brands)\n",
    "len(Brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProductDescription_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "ProductDescription_tags[0:4]\n",
    "\n",
    "for i in ProductDescription_tags:               #ieterating over web element of ProductDescription\n",
    "    ProductDescription=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    ProductDescriptions.append(ProductDescription) #appending each extected text into empty list\n",
    "ProductDescriptions[0:4]\n",
    "print(ProductDescriptions)\n",
    "\n",
    "len(ProductDescriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "#    print(location)\n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "print(Prices)\n",
    "len(Prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425777e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brands),len(ProductDescriptions),len(Prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87464691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Sunglass=pd.DataFrame({})\n",
    "Sunglass['Brands']=Brands\n",
    "Sunglass['ProductDescriptions']=ProductDescriptions\n",
    "Sunglass['Prices']=Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sunglass.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5ecf0",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99c427",
   "metadata": {},
   "source": [
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "    \n",
    "1. Rating\n",
    "\n",
    "2. Review summary\n",
    "\n",
    "3. Full review\n",
    "\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbe538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to fetch any url\n",
    "urls=driver.find_elements_by_xpath(\"//div[@class='_3UAT2v _16PBlm']\")\n",
    "urls[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls[0:4]: #giving range to print only top 4 data\n",
    "    i.click()\n",
    "#    print(i.get_attribute('href')) #url is present inside herf attribute and herf attribute is present inside a tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ratings=[]\n",
    "Reviewsummarys=[]\n",
    "Fullreviews=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65431941",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e87f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635029f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65207fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "\n",
    "Reviewsummary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "Reviewsummary_tags[0:4]\n",
    "\n",
    "for i in Reviewsummary_tags:               #ieterating over web element of Reviewsummary\n",
    "    Reviewsummary=i.text               #extracting text from each web element \n",
    "    Reviewsummarys.append(Reviewsummary) #appending each extected text into empty list\n",
    "Reviewsummarys[0:4]\n",
    "\n",
    "Fullreview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "Fullreview_tags[0:4]\n",
    "\n",
    "for i in Fullreview_tags:               #ieterating over web element of Fullreview\n",
    "    Fullreview=i.text               #extracting text from each web element \n",
    "    Fullreviews.append(Fullreview) #appending each extected text into empty list\n",
    "Fullreviews[0:4]\n",
    "\n",
    "print(len(Ratings),len(Reviewsummarys),len(Fullreviews))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iphone11=pd.DataFrame({})\n",
    "iphone11['Ratings']=Ratings\n",
    "iphone11['Reviewsummarys']=Reviewsummarys\n",
    "iphone11['Fullreviews']=Fullreviews\n",
    "iphone11.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48198134",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc239cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for Sunglasses search bar\n",
    "search_field_Sunglasses=driver.find_element_by_class_name(\"_3704LK\") # job search bar\n",
    "search_field_Sunglasses.send_keys(\"sneakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a20ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "ProductDescriptions=[]\n",
    "Prices=[]\n",
    "Offers=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318658e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "ProductDescription_tags=driver.find_elements_by_xpath(\"//div[@class='_2B099V']\")\n",
    "ProductDescription_tags[0:4]\n",
    "\n",
    "for i in ProductDescription_tags:               #ieterating over web element of ProductDescription\n",
    "    ProductDescription=i.text               #extracting text from each web element \n",
    "    ProductDescriptions.append(ProductDescription) #appending each extected text into empty list\n",
    "ProductDescriptions[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "Offer_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Offer_tags:               #ieterating over web element of Offer\n",
    "    Offer=i.text               #extracting text from each web element \n",
    "    Offers.append(Offer) #appending each extected text into empty list\n",
    "Offers[0:4]\n",
    "#print(Offers)\n",
    "\n",
    "\n",
    "print(len(Brands),len(ProductDescriptions),len(Prices),len(Offers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6440f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "ProductDescription_tags=driver.find_elements_by_xpath(\"//div[@class='_2B099V']\")\n",
    "ProductDescription_tags[0:4]\n",
    "\n",
    "for i in ProductDescription_tags:               #ieterating over web element of ProductDescription\n",
    "    ProductDescription=i.text               #extracting text from each web element \n",
    "    ProductDescriptions.append(ProductDescription) #appending each extected text into empty list\n",
    "ProductDescriptions[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "Offer_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Offer_tags:               #ieterating over web element of Offer\n",
    "    Offer=i.text               #extracting text from each web element \n",
    "    Offers.append(Offer) #appending each extected text into empty list\n",
    "Offers[0:4]\n",
    "#print(Offers)\n",
    "\n",
    "\n",
    "print(len(Brands),len(ProductDescriptions),len(Prices),len(Offers))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "ProductDescription_tags=driver.find_elements_by_xpath(\"//div[@class='_2B099V']\")\n",
    "ProductDescription_tags[0:4]\n",
    "\n",
    "for i in ProductDescription_tags:               #ieterating over web element of ProductDescription\n",
    "    ProductDescription=i.text               #extracting text from each web element \n",
    "    ProductDescriptions.append(ProductDescription) #appending each extected text into empty list\n",
    "ProductDescriptions[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "Offer_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Offer_tags:               #ieterating over web element of Offer\n",
    "    Offer=i.text               #extracting text from each web element \n",
    "    Offers.append(Offer) #appending each extected text into empty list\n",
    "Offers[0:4]\n",
    "#print(Offers)\n",
    "\n",
    "\n",
    "print(len(Brands),len(ProductDescriptions),len(Prices),len(Offers))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brands']=Brands\n",
    "sneakers['ProductDescriptions']=ProductDescriptions\n",
    "sneakers['Prices']=Prices\n",
    "#sneakers['Offers']=Offers\n",
    "sneakers.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7ed3c",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”, as shown inthe below image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8492bd6",
   "metadata": {},
   "source": [
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A7074.0_14049.0_7074.0%20TO%2014049.0%2C7119.0_14079.0_7119.0%20TO%2014079.0\n",
    "\n",
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A7074.0_14049.0_7074.0%20TO%2014049.0%2C7119.0_14079.0_7119.0%20TO%2014079.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "ShortShoes=[]\n",
    "Prices=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10621503",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "ShortShoe_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "ShortShoe_tags[0:4]\n",
    "\n",
    "for i in ShortShoe_tags:               #ieterating over web element of ShortShoe\n",
    "    ShortShoe=i.text               #extracting text from each web element \n",
    "    ShortShoes.append(ShortShoe) #appending each extected text into empty list\n",
    "ShortShoes[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "print(len(Brands),len(ShortShoes),len(Prices))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//li[@class='pagination-next']\")\n",
    "next_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91bd0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_tags[0:4]\n",
    "\n",
    "for i in Brand_tags:               #ieterating over web element of Brand name\n",
    "    Brand=i.text               #extracting text from each web element \n",
    "    Brands.append(Brand) #appending each extected text into empty list\n",
    "Brands[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "ShortShoe_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "ShortShoe_tags[0:4]\n",
    "\n",
    "for i in ShortShoe_tags:               #ieterating over web element of ShortShoe\n",
    "    ShortShoe=i.text               #extracting text from each web element \n",
    "    ShortShoes.append(ShortShoe) #appending each extected text into empty list\n",
    "ShortShoes[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "print(len(Brands),len(ShortShoes),len(Prices))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//li[@class='pagination-next']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Shoes=pd.DataFrame({})\n",
    "Shoes['Brands']=Brands\n",
    "Shoes['ShortShoes']=ShortShoes\n",
    "Shoes['Prices']=Prices\n",
    "Shoes.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2baa22",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a3f7d",
   "metadata": {},
   "source": [
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff5b35",
   "metadata": {},
   "source": [
    "# Intel Core i7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up flipkart.com website on automated chrome window\n",
    "driver.get('https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031&dc&crid=28SECK5JYJKXP&qid=1652336981&rnid=12598141031&sprefix=laptop%2Caps%2C333&ref=sr_nr_p_n_feature_thirteen_browse-bin_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles=[]\n",
    "Ratings=[]\n",
    "Prices=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44344a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_tags=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "Title_tags[0:4]\n",
    "\n",
    "for i in Title_tags:               #ieterating over web element of Brand name\n",
    "    Title=i.text               #extracting text from each web element \n",
    "    Titles.append(Title) #appending each extected text into empty list\n",
    "Titles[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "Rating_tags=driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of ShortShoe\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "print(len(Titles),len(Ratings),len(Prices))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e30791",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_tags=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "Title_tags[0:4]\n",
    "\n",
    "for i in Title_tags:               #ieterating over web element of Brand name\n",
    "    Title=i.text               #extracting text from each web element \n",
    "    Titles.append(Title) #appending each extected text into empty list\n",
    "Titles[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "Rating_tags=driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of ShortShoe\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "print(len(Titles),len(Ratings),len(Prices))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b41a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_tags=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "Title_tags[0:4]\n",
    "\n",
    "for i in Title_tags:               #ieterating over web element of Brand name\n",
    "    Title=i.text               #extracting text from each web element \n",
    "    Titles.append(Title) #appending each extected text into empty list\n",
    "Titles[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "Rating_tags=driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of ShortShoe\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "print(len(Titles),len(Ratings),len(Prices))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_tags=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "Title_tags[0:4]\n",
    "\n",
    "for i in Title_tags:               #ieterating over web element of Brand name\n",
    "    Title=i.text               #extracting text from each web element \n",
    "    Titles.append(Title) #appending each extected text into empty list\n",
    "Titles[0:4]\n",
    "#print(Brands)\n",
    "\n",
    "\n",
    "Rating_tags=driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of ShortShoe\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n",
    "#print(ProductDescriptions)\n",
    "\n",
    "\n",
    "Price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "Price_tags[0:4]\n",
    "\n",
    "for i in Price_tags:               #ieterating over web element of ProductDescription\n",
    "    Price=i.text               #extracting text from each web element \n",
    "    Prices.append(Price) #appending each extected text into empty list\n",
    "Prices[0:4]\n",
    "#print(Prices)\n",
    "\n",
    "print(len(Titles),len(Ratings),len(Prices))\n",
    "\n",
    "#code to click on nextf button\n",
    "next_button=driver.find_element_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "next_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IntelCorei7=pd.DataFrame({})\n",
    "IntelCorei7['Titles']=Titles\n",
    "IntelCorei7['Ratings']=Ratings\n",
    "IntelCorei7['Prices']=Prices\n",
    "IntelCorei7.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a97aa",
   "metadata": {},
   "source": [
    "# Intel Core i9 NOT SHOWING ON WEB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5cd6d",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace19618",
   "metadata": {},
   "source": [
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "\n",
    "2. Click on the Job option as shown in the image\n",
    "\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d8aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening up naukri.com website on automated chrome window\n",
    "driver.get('https://www.ambitionbox.com/jobs/search?designation=data-scientist&location=Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b024bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "JobTitles=[]\n",
    "CompanyNames=[]\n",
    "Ratings=[]\n",
    "Experiences=[]\n",
    "Salarys=[]\n",
    "Locations=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cd7533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JobTitles_tags=driver.find_elements_by_xpath(\"//a[@class='title noclick']\")\n",
    "JobTitles_tags[0:4]\n",
    "\n",
    "for i in JobTitles_tags:               #ieterating over web element of JobTitles\n",
    "    JobTitle=i.text               #extracting text from each web element \n",
    "    JobTitles.append(JobTitle) #appending each extected text into empty list\n",
    "JobTitles[0:4]\n",
    "len(JobTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ca7088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompanyNames_tags=driver.find_elements_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[2]/div[2]/div/div[1]/div[1]/div[2]/div/p\")\n",
    "CompanyNames_tags[0:4]\n",
    "\n",
    "for i in CompanyNames_tags:               #ieterating over web element of CompanyNames\n",
    "    CompanyName=i.text               #extracting text from each web element \n",
    "    CompanyNames.append(CompanyName) #appending each extected text into empty list\n",
    "CompanyNames[0:4]\n",
    "len(CompanyNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_tags=driver.find_elements_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[2]/div[1]/div/div[1]/div[2]/div[1]/div/a[1]\")\n",
    "Rating_tags[0:4]\n",
    "\n",
    "for i in Rating_tags:               #ieterating over web element of Rating\n",
    "    Rating=i.text               #extracting text from each web element \n",
    "    Ratings.append(Rating) #appending each extected text into empty list\n",
    "Ratings[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68213a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiences_tags=driver.find_elements_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[2]/div[2]/div/div[1]/div[1]/div[3]/div/div[1]/p\")\n",
    "Experiences_tags[0:4]\n",
    "\n",
    "for i in Experiences_tags:               #ieterating over web element of Experiences\n",
    "    Experience=i.text               #extracting text from each web element \n",
    "    Experiences.append(Experience) #appending each extected text into empty list\n",
    "Experiences[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e92b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Salarys_tags=driver.find_elements_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[2]/div[2]/div/div[1]/div[1]/div[3]/div/div[2]/p\")\n",
    "Salarys_tags[0:4]\n",
    "\n",
    "for i in Salarys_tags:               #ieterating over web element of Salary\n",
    "    Salary=i.text               #extracting text from each web element \n",
    "    Salarys.append(Salary) #appending each extected text into empty list\n",
    "Salarys[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Locations_tags=driver.find_elements_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[2]/div[2]/div/div[1]/div[1]/div[3]/div/div[3]/p\")\n",
    "Locations_tags[0:4]\n",
    "\n",
    "for i in Locations_tags:               #ieterating over web element of Location\n",
    "    Location=i.text               #extracting text from each web element \n",
    "    Locations.append(Location) #appending each extected text into empty list\n",
    "Locations[0:4]\n",
    "\n",
    "print(len(JobTitles),len(CompanyNames),len(Ratings),len(Experiences),len(Salarys),len(Locations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bac459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ambitionbox=pd.DataFrame({})\n",
    "ambitionbox['JobTitles']=JobTitles\n",
    "ambitionbox['CompanyNames']=CompanyNames\n",
    "ambitionbox['Ratings']=Ratings\n",
    "ambitionbox['Experiences']=Experiences\n",
    "ambitionbox['Salarys']=Salarys\n",
    "ambitionbox['Locations']=Locations\n",
    "\n",
    "ambitionbox.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1e5f8",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679e729",
   "metadata": {},
   "source": [
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "\n",
    "2. Click on the salaries option as shown in the image.\n",
    "\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "\n",
    "You have to scrape the data ticked in the above image.\n",
    "\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827839f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc09bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d289ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dae685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d81f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf76288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551bf83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41938962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845320fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2748ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
